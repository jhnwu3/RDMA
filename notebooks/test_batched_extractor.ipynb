{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "# Set the parent directory as the current directory\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Pairwise checks:\n",
      "31\n",
      "48\n",
      "Pairwise checks:\n",
      "16\n",
      "25\n",
      "Pairwise checks:\n",
      "19\n",
      "22\n",
      "Pairwise checks:\n",
      "15\n",
      "22\n",
      "Pairwise checks:\n",
      "10\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "from utils.data import read_json_file, print_json_structure\n",
    "hpo_data = read_json_file('data/dataset/mine_hpo.json')\n",
    "print(len(hpo_data))\n",
    "# print_json_structure(hpo_data)\n",
    "truth = hpo_data[\"53\"][\"phenotypes\"]\n",
    "# sample 5 texts and their ground truth phenotypes for testing.\n",
    "ids = [\"53\", \"54\", \"55\", \"56\", \"57\"] \n",
    "texts = []\n",
    "ground_truths = []\n",
    "for id in ids:\n",
    "    text = hpo_data[id][\"clinical_text\"]\n",
    "    texts.append(text)\n",
    "    truth = hpo_data[id][\"phenotypes\"]\n",
    "    ground_truth = []\n",
    "    for item in truth:\n",
    "        ground_truth.append(item[\"phenotype_name\"])\n",
    "    ground_truths.append(ground_truth)\n",
    "\n",
    "    sanity_check_list = []\n",
    "    for item in ground_truth:\n",
    "        # print(item)\n",
    "        if item in text:\n",
    "            sanity_check_list.append(item)\n",
    "    print(\"Pairwise checks:\")\n",
    "    print(len(sanity_check_list))\n",
    "    print(len(truth))\n",
    "# ground_truth = []\n",
    "# for item in truth:\n",
    "#     ground_truth.append(item[\"phenotype_name\"])\n",
    "# benchmark text runtime\n",
    "text = hpo_data[\"53\"][\"clinical_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnwu3/miniconda3/envs/hporag/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/johnwu3/miniconda3/envs/hporag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized ModelLoader with cache directory: /shared/rsaas/jw3/rare_disease/model_cache\n",
      "Loading LLM!\n",
      "Device configuration: cuda:0\n",
      "Using device map: {'': 'cuda:0'}\n",
      "Loading 70B model with quantization: mistral_24b\n",
      "Generated cache path: /shared/rsaas/jw3/rare_disease/model_cache/Mistral-Small-24B-Instruct-2501_4bit_nf4\n",
      "Valid cache found at /shared/rsaas/jw3/rare_disease/model_cache/Mistral-Small-24B-Instruct-2501_4bit_nf4\n",
      "Loading cached quantized model from /shared/rsaas/jw3/rare_disease/model_cache/Mistral-Small-24B-Instruct-2501_4bit_nf4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnwu3/miniconda3/envs/hporag/lib/python3.10/site-packages/transformers/quantizers/auto.py:206: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.94s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm here to help. How can I assist you today? If you have any medical questions or need information on a specific topic, feel free to ask. Please note that while I strive to provide accurate and helpful information, I am an AI and my knowledge cutoff is 2023, and I don't have real-time web browsing capabilities or personal experiences. For urgent medical concerns, always consult a healthcare professional.\n",
      "\n",
      "Here are a few examples of how I can assist you:\n",
      "\n",
      "* Explain medical terms or concepts\n",
      "* Provide information on diseases, symptoms, and treatments\n",
      "* Offer insights into medical procedures and tests\n",
      "* Discuss healthcare guidelines and recommendations\n",
      "* Answer questions related to biomedical research and studies\n",
      "\n",
      "What would you like to know or discuss?\n"
     ]
    }
   ],
   "source": [
    "def load_mistral_llm_client():\n",
    "    \"\"\"\n",
    "    Load a Mistral 24B LLM client configured with default cache directories\n",
    "    and assigned to cuda:0 device.\n",
    "    \n",
    "    Returns:\n",
    "        LocalLLMClient: Initialized LLM client for Mistral 24B\n",
    "    \"\"\"\n",
    "    from utils.llm_client import LocalLLMClient\n",
    "    \n",
    "    # Default cache directory from mine_hpo.py\n",
    "    default_cache_dir = \"/shared/rsaas/jw3/rare_disease/model_cache\"\n",
    "    \n",
    "    # Initialize and return the client with specific configuration\n",
    "    llm_client = LocalLLMClient(\n",
    "        model_type=\"mistral_24b\",  # Explicitly request mistral_24b model\n",
    "        device=\"cuda:0\",           # Assign to first GPU (cuda:0)\n",
    "        cache_dir=default_cache_dir,\n",
    "        temperature=0.0001           # Default temperature from mine_hpo.py\n",
    "    )\n",
    "    \n",
    "    return llm_client\n",
    "\n",
    "llm_client = load_mistral_llm_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model type: sentence_transformer\n",
      "Model name: abhinand/MedEmbed-small-v0.1\n",
      "Device: cuda:1\n",
      "Initializing SentenceTransformer with model: abhinand/MedEmbed-small-v0.1 on device: cuda:1\n",
      "Model successfully moved to cuda:1\n",
      "Verifying model by embedding sample text...\n",
      "Model initialized successfully. Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "from hporag.entity import RetrievalEnhancedEntityExtractor, RetrievalEnhancedEntityExtractorV2\n",
    "import json\n",
    "from hporag.entity import BaseEntityExtractor\n",
    "from utils.embedding import EmbeddingsManager\n",
    "import numpy as np\n",
    "# Load system prompts\n",
    "with open('data/prompts/system_prompts.json', 'r') as f:\n",
    "    prompts = json.load(f)\n",
    "    system_message_extraction = prompts.get(\"system_message_I\", \"\")\n",
    "    system_message_verification = prompts.get(\"system_message_II\", \"\")\n",
    "\n",
    "# Initialize embedding manager with MedEmbed using sentence transformers\n",
    "embedding_manager = EmbeddingsManager(\n",
    "    model_type=\"sentence_transformer\",\n",
    "    model_name=\"abhinand/MedEmbed-small-v0.1\",  # Medical-domain sentence transformer model\n",
    "    device=\"cuda:1\"\n",
    ")\n",
    "\n",
    "# Load embeddings\n",
    "embedded_documents = np.load('data/vector_stores/G2GHPO_metadata_medembed.npy', allow_pickle=True)\n",
    "llm_client.temperature = 0.001  # Lower temperature for more precise extraction\n",
    "# Initialize the retrieval-enhanced extractor\n",
    "retrieval_extractor = RetrievalEnhancedEntityExtractor(\n",
    "    llm_client=llm_client,\n",
    "    embedding_manager=embedding_manager,\n",
    "    embedded_documents=embedded_documents,\n",
    "    system_message=system_message_extraction,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "retrieval_extractor_v2 = RetrievalEnhancedEntityExtractorV2(\n",
    "    llm_client=llm_client,\n",
    "    embedding_manager=embedding_manager,\n",
    "    embedded_documents=embedded_documents,\n",
    "    system_message=system_message_extraction,\n",
    "    max_batch_size=32,\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1m24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "retr_v1 = retrieval_extractor.extract_entities(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_batch_size = 48, 1m 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retr_v2 = retrieval_extractor_v2.extract_entities(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "print(len(retr_v1))\n",
    "print(len(retr_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "def intersection_method1(list1, list2):\n",
    "    return list(set(list1) & set(list2))\n",
    "print(len(intersection_method1(retr_v1, retr_v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['basket weaving pattern', 'basket weaving pattern under EM']\n"
     ]
    }
   ],
   "source": [
    "def disjoint_elements(list1, list2):\n",
    "    # Convert to sets for efficient operations\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    \n",
    "    # Symmetric difference finds elements in either set, but not in both\n",
    "    return list(set1 ^ set2)\n",
    "\n",
    "# Example usage\n",
    "a = [1, 2, 3, 4, 5]\n",
    "b = [4, 5, 6, 7, 8]\n",
    "\n",
    "print(disjoint_elements(retr_v1, retr_v2))  # [1, 2, 3, 6, 7, 8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hporag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
